{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  merchant_id coupon_id discount_rate distance date_received  \\\n",
      "0  1439408         2632      null          null        0          null   \n",
      "1  1439408         4663     11002        150:20        1      20160528   \n",
      "2  1439408         2632      8591          20:1        0      20160217   \n",
      "3  1439408         2632      1078          20:1        0      20160319   \n",
      "4  1439408         2632      8591          20:1        0      20160613   \n",
      "\n",
      "       date  \n",
      "0  20160217  \n",
      "1      null  \n",
      "2      null  \n",
      "3      null  \n",
      "4      null  \n",
      "   user_id  merchant_id  coupon_id discount_rate distance  date_received\n",
      "0  4129537          450       9983          30:5        1       20160712\n",
      "1  6949378         1300       3429          30:5     null       20160706\n",
      "2  2166529         7113       6928        200:20        5       20160727\n",
      "3  2166529         7113       1808        100:10        5       20160727\n",
      "4  6172162         7605       6500          30:1        2       20160708\n"
     ]
    }
   ],
   "source": [
    "off_train = pd.read_csv('data\\\\ccf_offline_stage1_train.csv')\n",
    "off_train.columns = ['user_id','merchant_id','coupon_id','discount_rate','distance','date_received','date']\n",
    "print (off_train.head())\n",
    "off_test = pd.read_csv('data\\\\ccf_offline_stage1_test_revised.csv',header = 0,names = ['user_id','merchant_id','coupon_id','discount_rate','distance','date_received'])\n",
    "print (off_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = off_test\n",
    "feature3 = off_train[((off_train.date >= '20160315')&(off_train.date<= '20160630'))|((off_train.date_received>= '20160315')& (off_train.date_received <= '20160630')& (off_train.date == 'null'))]\n",
    "dataset2 = off_train[(off_train.date_received >= '20160515')& (off_train.date_received <= '20160615')]\n",
    "feature2 = off_train[((off_train.date >= '20160201')&(off_train.date<= '20160514'))|((off_train.date_received>= '20160201')& (off_train.date_received <= '20160514')& (off_train.date == 'null'))]\n",
    "dataset1 = off_train[(off_train.date_received >= '20160414')& (off_train.date_received <= '20160514')]\n",
    "feature1 = off_train[((off_train.date >= '20160101')&(off_train.date<= '20160413'))|((off_train.date_received>= '20160101')& (off_train.date_received <= '20160413')& (off_train.date == 'null'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# coupon related feature   #############\n",
    "'''\n",
    "coupon related feature:is_man_jian,discount_man, \n",
    "    discount_jian, count_rate,  day_of_month, day of week, days_distance, is_weekend\n",
    "总共九个特征\n",
    "\n",
    "'''\n",
    "def is_man_jian(s):\n",
    "    s = str(s)\n",
    "    s = s.split(':')\n",
    "    if len(s) == 1:\n",
    "        return 0   #代表该优惠为折扣率\n",
    "    else: \n",
    "        return 1 #代表该优惠为满减 \n",
    "def cal_discount_rate(s):\n",
    "    s = str(s)\n",
    "    s = s.split(':')\n",
    "    if len(s)==1:\n",
    "        return float(s[0])\n",
    "    else:\n",
    "        return 1.0- float(s[1])/float(s[0])\n",
    "def get_discount_man(s):\n",
    "    s = str(s)\n",
    "    s = s.split(':')\n",
    "    if len(s)== 1:\n",
    "        return 'null'\n",
    "    else:\n",
    "        return int(s[0])\n",
    "def get_discount_jian(s):\n",
    "    s = str(s)\n",
    "    s = s.split(':')\n",
    "    if len(s)== 1:\n",
    "        return 'null'\n",
    "    else:\n",
    "        return int(s[1])    \n",
    "def if_weekend(s):\n",
    "    s = int(s)\n",
    "    if s == 7 | s == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def cal_label_coupon_feature_buy_count(s, df):\n",
    "    if s in df.index:\n",
    "        return df.loc[s, 'label_coupon_feature_buy_count']\n",
    "    else:\n",
    "        return 0\n",
    "def cal_label_coupon_feature_receive_count(s, df):\n",
    "    if s in df.index:\n",
    "        return df.loc[s, 'label_coupon_feature_receive_count']\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "d:\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#提取是否是满减\n",
    "dataset3['is_man_jian'] = dataset3.discount_rate.astype('str').apply(is_man_jian).copy()\n",
    "dataset3['discount_man'] = dataset3.discount_rate.astype('str').apply(get_discount_man).copy()\n",
    "dataset3['discount_jian'] = dataset3.discount_rate.astype('str').apply(get_discount_jian).copy()\n",
    "dataset3['discount_rate'] = dataset3.discount_rate.astype('str').apply(cal_discount_rate).copy()\n",
    "dataset3['day_of_month'] = dataset3.date_received.astype('str').apply(lambda x: int(x[6:8])).copy()\n",
    "dataset3['day_of_week'] = dataset3.date_received.astype('str').apply(lambda x: date(int(x[0:4]), int(x[4:6]), int(x[6:8])).weekday()+1).copy()\n",
    "dataset3['days_distance'] = dataset3.date_received.astype('str').apply(lambda x: (date(int(x[0:4]), int(x[4:6]), int(x[6:8])) - date(2016, 6, 30)).days).copy()\n",
    "dataset3['is_weekend'] = dataset3.day_of_week.astype('int').apply(if_weekend).copy()\n",
    "\n",
    "t = feature3[feature3.date != 'null'][['coupon_id']].copy()\n",
    "t['label_coupon_feature_buy_count'] = 1\n",
    "t = t.groupby('coupon_id').agg(sum)\n",
    "dataset3['label_coupon_feature_buy_count'] = dataset3.coupon_id.astype('str').apply(cal_label_coupon_feature_buy_count, df=t)\n",
    "\n",
    "t1 = feature3[feature3.date_received != 'null'][['coupon_id']].copy()\n",
    "t1['label_coupon_feature_receive_count'] = 1\n",
    "t1 = t1.groupby('coupon_id').agg(sum)\n",
    "dataset3['label_coupon_feature_receive_count'] = dataset3.coupon_id.astype('str').apply(cal_label_coupon_feature_receive_count, df=t1)\n",
    "dataset3['label_coupon_feature_rate'] = dataset3.label_coupon_feature_buy_count.astype('float')/dataset3.label_coupon_feature_receive_count\n",
    "\n",
    " \n",
    "#计算优惠券的数量\n",
    "d = dataset3[['coupon_id']].copy()\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()#将index中内容转换到column中\n",
    "dataset3 = pd.merge(dataset3, d, on='coupon_id', how ='left')\n",
    "dataset3.to_csv('data1/coupon3_feature.csv', index= None) \n",
    "\n",
    "#dataset2\n",
    "dataset2['is_man_jian'] = dataset2.discount_rate.astype('str').apply(is_man_jian).copy()\n",
    "dataset2['discount_man'] = dataset2.discount_rate.astype('str').apply(get_discount_man).copy()\n",
    "dataset2['discount_jian'] = dataset2.discount_rate.astype('str').apply(get_discount_jian).copy()\n",
    "dataset2['discount_rate'] = dataset2.discount_rate.astype('str').apply(cal_discount_rate).copy()\n",
    "dataset2['day_of_month'] = dataset2.date_received.astype('str').apply(lambda x: int(x[6:8])).copy()\n",
    "dataset2['day_of_week'] = dataset2.date_received.astype('str').apply(lambda x: date(int(x[0:4]), int(x[4:6]), int(x[6:8])).weekday()+1).copy()\n",
    "dataset2['days_distance'] = dataset2.date_received.astype('str').apply(lambda x: (date(int(x[0:4]), int(x[4:6]), int(x[6:8])) - date(2016, 5, 14)).days).copy()\n",
    "dataset2['is_weekend'] = dataset2.day_of_week.astype('int').apply(if_weekend).copy()\n",
    "\n",
    "t = feature2[feature2.date != 'null'][['coupon_id']].copy()\n",
    "t['label_coupon_feature_buy_count'] = 1\n",
    "t = t.groupby('coupon_id').agg(sum)\n",
    "dataset2['label_coupon_feature_buy_count'] = dataset2.coupon_id.astype('str').apply(cal_label_coupon_feature_buy_count, df=t)\n",
    "\n",
    "t1 = feature2[feature2.date_received != 'null'][['coupon_id']].copy()\n",
    "t1['label_coupon_feature_receive_count'] = 1\n",
    "t1 = t1.groupby('coupon_id').agg(sum)\n",
    "dataset2['label_coupon_feature_receive_count'] = dataset2.coupon_id.astype('str').apply(cal_label_coupon_feature_receive_count, df=t1)\n",
    "dataset2['label_coupon_feature_rate'] = dataset2.label_coupon_feature_buy_count.astype('float')/dataset2.label_coupon_feature_receive_count\n",
    "#计算优惠券的数量\n",
    "d = dataset2[['coupon_id']].copy()\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()#将index中内容转换到column中\n",
    "dataset2 = pd.merge(dataset2, d, on='coupon_id', how ='left')\n",
    "dataset2.to_csv('data1/coupon2_feature.csv', index= None)\n",
    "\n",
    "\n",
    "dataset1['is_man_jian'] = dataset1.discount_rate.astype('str').apply(is_man_jian).copy()\n",
    "dataset1['discount_man'] = dataset1.discount_rate.astype('str').apply(get_discount_man).copy()\n",
    "dataset1['discount_jian'] = dataset1.discount_rate.astype('str').apply(get_discount_jian).copy()\n",
    "dataset1['discount_rate'] = dataset1.discount_rate.astype('str').apply(cal_discount_rate).copy()\n",
    "dataset1['day_of_month'] = dataset1.date_received.astype('str').apply(lambda x: int(x[6:8])).copy()\n",
    "dataset1['day_of_week'] = dataset1.date_received.astype('str').apply(lambda x: date(int(x[0:4]), int(x[4:6]), int(x[6:8])).weekday()+1).copy()\n",
    "dataset1['days_distance'] = dataset1.date_received.astype('str').apply(lambda x: (date(int(x[0:4]), int(x[4:6]), int(x[6:8])) - date(2016, 4, 13)).days).copy()\n",
    "dataset1['is_weekend'] = dataset1.day_of_week.astype('int').apply(if_weekend).copy()\n",
    "t = feature1[feature1.date != 'null'][['coupon_id']].copy()\n",
    "t['label_coupon_feature_buy_count'] = 1\n",
    "t = t.groupby('coupon_id').agg(sum)\n",
    "dataset1['label_coupon_feature_buy_count'] = dataset1.coupon_id.astype('str').apply(cal_label_coupon_feature_buy_count, df=t)\n",
    "\n",
    "t1 = feature1[feature1.date_received != 'null'][['coupon_id']].copy()\n",
    "t1['label_coupon_feature_receive_count'] = 1\n",
    "t1 = t1.groupby('coupon_id').agg(sum)\n",
    "dataset1['label_coupon_feature_receive_count'] = dataset1.coupon_id.astype('str').apply(cal_label_coupon_feature_receive_count, df=t1)\n",
    "dataset1['label_coupon_feature_rate'] = dataset1.label_coupon_feature_buy_count.astype('float')/dataset1.label_coupon_feature_receive_count\n",
    "#计算优惠券的数量\n",
    "d = dataset1[['coupon_id']].copy()\n",
    "d['coupon_count'] = 1\n",
    "d = d.groupby('coupon_id').agg('sum').reset_index()#将index中内容转换到column中\n",
    "dataset1 = pd.merge(dataset1, d, on='coupon_id', how ='left')\n",
    "dataset1.to_csv('data1/coupon1_feature.csv', index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# merchant related feature   #############\n",
    "\"\"\"\n",
    "1.merchant related: \n",
    "      total_sales. sales_use_coupon.  total_coupon\n",
    "      coupon_rate = sales_use_coupon/total_sales.  \n",
    "      transfer_rate = sales_use_coupon/total_coupon. \n",
    "      merchant_avg_distance,merchant_min_distance,merchant_max_distance of those use coupon\n",
    "增加 消费过该商家的不同用户数量  merchant_user_buy_count\n",
    "\"\"\"\n",
    "merchant3 = feature3[['user_id', 'merchant_id', 'coupon_id', 'distance', 'date_received', 'date']].copy()\n",
    "t = merchant3[['merchant_id']].copy()\n",
    "t.drop_duplicates(inplace = True)\n",
    "\n",
    "#商家被消费次数\n",
    "t1 = merchant3[merchant3.date != 'null'][['merchant_id']].copy()\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "#商家使用优惠券被消费次数\n",
    "t2 = merchant3[(merchant3.date!='null') & (merchant3.coupon_id!= 'null')][['merchant_id']].copy()\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg(sum).reset_index()\n",
    "#商家发放优惠券次数\n",
    "t3 = merchant3[merchant3.coupon_id!='null'][['merchant_id']].copy()\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "#使用优惠券消费的用户与商家最小距离\n",
    "t4 = merchant3[(merchant3.date!='null') & (merchant3.coupon_id!= 'null')][['merchant_id','distance']].copy()\n",
    "t4.replace('null', -1, inplace = True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1, np.nan, inplace = True)\n",
    "t5 = t4.groupby('merchant_id').agg(min).reset_index()\n",
    "t5.rename(columns = {'distance':'merchant_min_distance'}, inplace = True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg(max).reset_index()\n",
    "t6.rename(columns = {'distance':'merchant_max_distance'}, inplace = True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns = {'distance':'merchant_mean_distance'}, inplace = True)\n",
    "\n",
    "t8 = merchant3[['merchant_id', 'user_id']].copy()\n",
    "t8.drop_duplicates(inplace = True)\n",
    "t8['merchant_user_buy_count'] = 1\n",
    "t8 = t8.groupby('merchant_id').agg(sum).reset_index()\n",
    "t8 = t8[['merchant_id','merchant_user_buy_count']].copy()\n",
    "\n",
    "merchant3_feature = pd.merge(t, t1, on ='merchant_id', how= 'left')\n",
    "merchant3_feature = pd.merge(merchant3_feature, t2, on ='merchant_id', how= 'left')\n",
    "merchant3_feature = pd.merge(merchant3_feature, t3, on ='merchant_id', how= 'left')\n",
    "merchant3_feature = pd.merge(merchant3_feature, t5, on ='merchant_id', how= 'left')\n",
    "merchant3_feature = pd.merge(merchant3_feature, t6, on ='merchant_id', how= 'left')\n",
    "merchant3_feature = pd.merge(merchant3_feature, t7, on ='merchant_id', how= 'left')\n",
    "merchant3_feature = pd.merge(merchant3_feature, t8, on ='merchant_id', how= 'left')\n",
    "merchant3_feature.sales_use_coupon = merchant3_feature.sales_use_coupon.replace(np.nan, 0)\n",
    "merchant3_feature['merchant_coupon_transfer_rate'] = merchant3_feature.sales_use_coupon.astype('float') / merchant3_feature.total_coupon\n",
    "merchant3_feature['coupon_rate'] = merchant3_feature.sales_use_coupon.astype('float') / merchant3_feature.total_sales\n",
    "merchant3_feature.total_coupon = merchant3_feature.total_coupon.replace(np.nan, 0)\n",
    "\n",
    "merchant3_feature.to_csv('data1/merchant3_feature.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   merchant_id  sales_use_coupon\n",
      "0           15                11\n",
      "1           17                 1\n",
      "2           18                 2\n",
      "3           20                 3\n",
      "4           21                 1\n"
     ]
    }
   ],
   "source": [
    "merchant2 = feature2[['user_id','merchant_id', 'coupon_id', 'distance', 'date_received', 'date']].copy()\n",
    "t = merchant2[['merchant_id']].copy()\n",
    "t.drop_duplicates(inplace = True)\n",
    "\n",
    "#商家被消费次数\n",
    "t1 = merchant2[merchant2.date != 'null'][['merchant_id']].copy()\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "#商家使用优惠券被消费次数\n",
    "t2 = merchant2[(merchant2.date!='null') & (merchant2.coupon_id!= 'null')][['merchant_id']].copy()\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg(sum).reset_index()\n",
    "print(t2.head())\n",
    "#商家发放优惠券次数\n",
    "t3 = merchant2[merchant2.coupon_id!='null'][['merchant_id']].copy()\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "#使用优惠券消费的用户与商家最小距离\n",
    "t4 = merchant2[(merchant2.date!='null') & (merchant2.coupon_id!= 'null')][['merchant_id','distance']].copy()\n",
    "t4.replace('null', -1, inplace = True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1, np.nan, inplace = True)\n",
    "t5 = t4.groupby('merchant_id').agg(min).reset_index()\n",
    "t5.rename(columns = {'distance':'merchant_min_distance'}, inplace = True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg(max).reset_index()\n",
    "t6.rename(columns = {'distance':'merchant_max_distance'}, inplace = True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns = {'distance':'merchant_mean_distance'}, inplace = True)\n",
    "\n",
    "t8 = merchant2[['merchant_id', 'user_id']].copy()\n",
    "t8.drop_duplicates(inplace = True)\n",
    "t8['merchant_user_buy_count'] = 1\n",
    "t8 = t8.groupby('merchant_id').agg(sum).reset_index()\n",
    "t8 = t8[['merchant_id','merchant_user_buy_count']].copy()\n",
    "\n",
    "merchant2_feature = pd.merge(t, t1, on ='merchant_id', how= 'left')\n",
    "merchant2_feature = pd.merge(merchant2_feature, t2, on ='merchant_id', how= 'left')\n",
    "merchant2_feature = pd.merge(merchant2_feature, t3, on ='merchant_id', how= 'left')\n",
    "merchant2_feature = pd.merge(merchant2_feature, t5, on ='merchant_id', how= 'left')\n",
    "merchant2_feature = pd.merge(merchant2_feature, t6, on ='merchant_id', how= 'left')\n",
    "merchant2_feature = pd.merge(merchant2_feature, t7, on ='merchant_id', how= 'left')\n",
    "merchant2_feature = pd.merge(merchant2_feature, t8, on ='merchant_id', how= 'left')\n",
    "merchant2_feature.sales_use_coupon = merchant2_feature.sales_use_coupon.replace(np.nan, 0)\n",
    "merchant2_feature['merchant_coupon_transfer_rate'] = merchant2_feature.sales_use_coupon.astype('float') / merchant2_feature.total_coupon\n",
    "merchant2_feature['coupon_rate'] = merchant2_feature.sales_use_coupon.astype('float') / merchant2_feature.total_sales\n",
    "merchant2_feature.total_coupon = merchant2_feature.total_coupon.replace(np.nan, 0)\n",
    "\n",
    "merchant2_feature.to_csv('data1/merchant2_feature.csv',index = None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merchant1 = feature1[['user_id','merchant_id', 'coupon_id', 'distance', 'date_received', 'date']].copy()\n",
    "t = merchant1[['merchant_id']].copy()\n",
    "t.drop_duplicates(inplace = True)\n",
    "\n",
    "#商家被消费次数\n",
    "t1 = merchant1[merchant1.date != 'null'][['merchant_id']].copy()\n",
    "t1['total_sales'] = 1\n",
    "t1 = t1.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "#商家使用优惠券被消费次数\n",
    "t2 = merchant1[(merchant1.date!='null') & (merchant1.coupon_id!= 'null')][['merchant_id']].copy()\n",
    "t2['sales_use_coupon'] = 1\n",
    "t2 = t2.groupby('merchant_id').agg(sum).reset_index()\n",
    "#商家发放优惠券次数\n",
    "t3 = merchant1[merchant1.coupon_id!='null'][['merchant_id']].copy()\n",
    "t3['total_coupon'] = 1\n",
    "t3 = t3.groupby('merchant_id').agg('sum').reset_index()\n",
    "\n",
    "#使用优惠券消费的用户与商家最小距离\n",
    "t4 = merchant1[(merchant1.date!='null') & (merchant1.coupon_id!= 'null')][['merchant_id','distance']].copy()\n",
    "t4.replace('null', -1, inplace = True)\n",
    "t4.distance = t4.distance.astype('int')\n",
    "t4.replace(-1, np.nan, inplace = True)\n",
    "t5 = t4.groupby('merchant_id').agg(min).reset_index()\n",
    "t5.rename(columns = {'distance':'merchant_min_distance'}, inplace = True)\n",
    "\n",
    "t6 = t4.groupby('merchant_id').agg(max).reset_index()\n",
    "t6.rename(columns = {'distance':'merchant_max_distance'}, inplace = True)\n",
    "\n",
    "t7 = t4.groupby('merchant_id').agg('mean').reset_index()\n",
    "t7.rename(columns = {'distance':'merchant_mean_distance'}, inplace = True)\n",
    "\n",
    "t8 = merchant1[['merchant_id', 'user_id']].copy()\n",
    "t8.drop_duplicates(inplace = True)\n",
    "t8['merchant_user_buy_count'] = 1\n",
    "t8 = t8.groupby('merchant_id').agg(sum).reset_index()\n",
    "t8 = t8[['merchant_id','merchant_user_buy_count']].copy()\n",
    "\n",
    "merchant1_feature = pd.merge(t, t1, on ='merchant_id', how= 'left')\n",
    "merchant1_feature = pd.merge(merchant1_feature, t2, on ='merchant_id', how= 'left')\n",
    "merchant1_feature = pd.merge(merchant1_feature, t3, on ='merchant_id', how= 'left')\n",
    "merchant1_feature = pd.merge(merchant1_feature, t5, on ='merchant_id', how= 'left')\n",
    "merchant1_feature = pd.merge(merchant1_feature, t6, on ='merchant_id', how= 'left')\n",
    "merchant1_feature = pd.merge(merchant1_feature, t7, on ='merchant_id', how= 'left')\n",
    "merchant1_feature = pd.merge(merchant1_feature, t8, on ='merchant_id', how= 'left')\n",
    "merchant1_feature.sales_use_coupon = merchant1_feature.sales_use_coupon.replace(np.nan, 0)\n",
    "merchant1_feature['merchant_coupon_transfer_rate'] = merchant1_feature.sales_use_coupon.astype('float') / merchant1_feature.total_coupon\n",
    "merchant1_feature['coupon_rate'] = merchant1_feature.sales_use_coupon.astype('float') / merchant1_feature.total_sales\n",
    "merchant1_feature.total_coupon = merchant1_feature.total_coupon.replace(np.nan, 0)\n",
    "\n",
    "merchant1_feature.to_csv('data1/merchant1_feature.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_date_datereceived_gap(s):\n",
    "    s = s.split(':')\n",
    "    return (date(int(s[0][0:4]),int(s[0][4:6]),int(s[0][6:8])) - date(int(s[1][0:4]), int(s[1][4:6]), int(s[1][6:8]))).days\n",
    "############# user related feature   #############\n",
    "\"\"\"\n",
    "3.user related: \n",
    "      count_merchant. \n",
    "      user_avg_distance, user_min_distance,user_max_distance. \n",
    "      buy_use_coupon. buy_total. coupon_received.\n",
    "      buy_use_coupon/coupon_received. \n",
    "      buy_use_coupon/buy_total\n",
    "      user_date_datereceived_gap      \n",
    "\"\"\"\n",
    "#for dataset3\n",
    "\n",
    "user3 = feature3[['user_id', 'merchant_id', 'coupon_id', 'discount_rate','distance', 'date_received', 'date']]\n",
    "t = user3[['user_id']].copy()\n",
    "t.drop_duplicates(inplace = True)\n",
    "#用户消费商户数量\n",
    "t1 = user3[user3.date != 'null'][['user_id', 'merchant_id']].copy()\n",
    "t1.drop_duplicates(inplace = True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg(sum).reset_index()\n",
    "t1.rename(columns = {'merchant_id': 'count_merchant'}, inplace = True)\n",
    "#用户消费次数\n",
    "t2 = user3[user3.date != 'null'][['user_id']].copy()\n",
    "t2['buy_total'] = 1\n",
    "t2 = t2.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#用户使用优惠券消费次数\n",
    "t3 = user3[(user3.date != 'null') & (user3.coupon_id!= 'null')][['user_id']].copy()\n",
    "t3['buy_use_coupon'] = 1\n",
    "t3 = t3.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#用户领取优惠券次数\n",
    "t4 = user3[user3.coupon_id != 'null'][['user_id']].copy()\n",
    "t4['coupon_received'] = 1\n",
    "t4 = t4.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#所有使用优惠券消费的商户与用户最大、最小、平均距离\n",
    "t5 = user3[(user3.coupon_id!= 'null') & (user3.date !='null')][['user_id', 'distance']].copy()\n",
    "t5.replace('null', -1, inplace = True)\n",
    "t5.distance = t5.distance.astype('int')\n",
    "t5.replace(-1, np.nan, inplace = True)\n",
    "t6 = t5.groupby('user_id').agg(min).reset_index()\n",
    "t6.rename(columns = {'distance':'user_min_distance'}, inplace = True)\n",
    "\n",
    "t7 = t5.groupby('user_id').agg(max).reset_index()\n",
    "t7.rename(columns = {'distance':'user_max_distance'}, inplace = True)\n",
    "\n",
    "t8 = t5.groupby('user_id').agg('mean').reset_index()\n",
    "t8.rename(columns = {'distance':'user_mean_distance'}, inplace = True)\n",
    "\n",
    "t9 = user3[(user3.date!='null')&(user3.date_received !='null')][['user_id', 'date_received', 'date']].copy()\n",
    "t9['user_date_datereceived_gap'] = t9.date + ':' +t9.date_received\n",
    "t9.user_date_datereceived_gap = t9.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t9 = t9[['user_id', 'user_date_datereceived_gap']].copy()\n",
    "\n",
    "t10 = t9.groupby('user_id').agg(min).reset_index()\n",
    "t10.rename(columns = {'user_date_datereceived_gap':'min_user_date_datereceived_gap'}, inplace=True)\n",
    "                                                                \n",
    "t11 = t9.groupby('user_id').agg(max).reset_index()\n",
    "t11.rename(columns = {'user_date_datereceived_gap':'max_user_date_datereceived_gap'}, inplace=True)\n",
    "t12 = t9.groupby('user_id').agg('mean').reset_index()\n",
    "t12.rename(columns = {'user_date_datereceived_gap':'avg_user_date_datereceived_gap'}, inplace=True)\n",
    "\n",
    "user3_feature = pd.merge(t,t1,on='user_id',how='left')                                                               \n",
    "user3_feature = pd.merge(user3_feature,t2,on='user_id',how='left') \n",
    "user3_feature = pd.merge(user3_feature,t3,on='user_id',how='left') \n",
    "user3_feature = pd.merge(user3_feature,t4,on='user_id',how='left') \n",
    "user3_feature = pd.merge(user3_feature,t6,on='user_id',how='left') \n",
    "user3_feature = pd.merge(user3_feature,t7,on='user_id',how='left') \n",
    "user3_feature = pd.merge(user3_feature,t8,on='user_id',how='left')  \n",
    "user3_feature = pd.merge(user3_feature,t10,on='user_id',how='left')    \n",
    "user3_feature = pd.merge(user3_feature,t11,on='user_id',how='left')        \n",
    "user3_feature = pd.merge(user3_feature,t12,on='user_id',how='left')  \n",
    "user3_feature.count_merchant = user3_feature.count_merchant.replace(np.nan,0)\n",
    "user3_feature.buy_use_coupon = user3_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user3_feature['buy_use_coupon_rate'] = user3_feature.buy_use_coupon.astype('float') / user3_feature.buy_total.astype('float')\n",
    "user3_feature['user_coupon_transfer_rate'] = user3_feature.buy_use_coupon.astype('float') / user3_feature.coupon_received.astype('float')\n",
    "user3_feature.buy_total = user3_feature.buy_total.replace(np.nan,0)\n",
    "user3_feature.coupon_received = user3_feature.coupon_received.replace(np.nan,0)\n",
    "user3_feature.to_csv('data1/user3_feature.csv',index=None) \n",
    "\n",
    "\n",
    "#for dataset2\n",
    "\n",
    "user2 = feature2[['user_id', 'merchant_id', 'coupon_id', 'discount_rate','distance', 'date_received', 'date']]\n",
    "t = user2[['user_id']].copy()\n",
    "t.drop_duplicates(inplace = True)\n",
    "#用户消费商户数量\n",
    "t1 = user2[user2.date != 'null'][['user_id', 'merchant_id']].copy()\n",
    "t1.drop_duplicates(inplace = True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg(sum).reset_index()\n",
    "t1.rename(columns = {'merchant_id': 'count_merchant'}, inplace = True)\n",
    "#用户消费次数\n",
    "t2 = user2[user2.date != 'null'][['user_id']].copy()\n",
    "t2['buy_total'] = 1\n",
    "t2 = t2.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#用户使用优惠券消费次数\n",
    "t3 = user2[(user2.date != 'null') & (user2.coupon_id!= 'null')][['user_id']].copy()\n",
    "t3['buy_use_coupon'] = 1\n",
    "t3 = t3.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#用户领取优惠券次数\n",
    "t4 = user2[user2.coupon_id != 'null'][['user_id']].copy()\n",
    "t4['coupon_received'] = 1\n",
    "t4 = t4.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#所有使用优惠券消费的商户与用户最大、最小、平均距离\n",
    "t5 = user2[(user2.coupon_id!= 'null') & (user2.date !='null')][['user_id', 'distance']].copy()\n",
    "t5.replace('null', -1, inplace = True)\n",
    "t5.distance = t5.distance.astype('int')\n",
    "t5.replace(-1, np.nan, inplace = True)\n",
    "t6 = t5.groupby('user_id').agg(min).reset_index()\n",
    "t6.rename(columns = {'distance':'user_min_distance'}, inplace = True)\n",
    "\n",
    "t7 = t5.groupby('user_id').agg(max).reset_index()\n",
    "t7.rename(columns = {'distance':'user_max_distance'}, inplace = True)\n",
    "\n",
    "t8 = t5.groupby('user_id').agg('mean').reset_index()\n",
    "t8.rename(columns = {'distance':'user_mean_distance'}, inplace = True)\n",
    "\n",
    "t9 = user2[(user2.date!='null')&(user2.date_received !='null')][['user_id', 'date_received', 'date']].copy()\n",
    "t9['user_date_datereceived_gap'] = t9.date + ':' +t9.date_received\n",
    "t9.user_date_datereceived_gap = t9.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t9 = t9[['user_id', 'user_date_datereceived_gap']].copy()\n",
    "\n",
    "t10 = t9.groupby('user_id').agg(min).reset_index()\n",
    "t10.rename(columns = {'user_date_datereceived_gap':'min_user_date_datereceived_gap'}, inplace=True)\n",
    "                                                                \n",
    "t11 = t9.groupby('user_id').agg(max).reset_index()\n",
    "t11.rename(columns = {'user_date_datereceived_gap':'max_user_date_datereceived_gap'}, inplace=True)\n",
    "t12 = t9.groupby('user_id').agg('mean').reset_index()\n",
    "t12.rename(columns = {'user_date_datereceived_gap':'avg_user_date_datereceived_gap'}, inplace=True)\n",
    "\n",
    "user2_feature = pd.merge(t,t1,on='user_id',how='left')                                                               \n",
    "user2_feature = pd.merge(user2_feature,t2,on='user_id',how='left') \n",
    "user2_feature = pd.merge(user2_feature,t3,on='user_id',how='left') \n",
    "user2_feature = pd.merge(user2_feature,t4,on='user_id',how='left') \n",
    "user2_feature = pd.merge(user2_feature,t6,on='user_id',how='left') \n",
    "user2_feature = pd.merge(user2_feature,t7,on='user_id',how='left') \n",
    "user2_feature = pd.merge(user2_feature,t8,on='user_id',how='left')  \n",
    "user2_feature = pd.merge(user2_feature,t10,on='user_id',how='left')    \n",
    "user2_feature = pd.merge(user2_feature,t11,on='user_id',how='left')        \n",
    "user2_feature = pd.merge(user2_feature,t12,on='user_id',how='left')  \n",
    "user2_feature.count_merchant = user2_feature.count_merchant.replace(np.nan,0)\n",
    "user2_feature.buy_use_coupon = user2_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user2_feature['buy_use_coupon_rate'] = user2_feature.buy_use_coupon.astype('float') / user2_feature.buy_total.astype('float')\n",
    "user2_feature['user_coupon_transfer_rate'] = user2_feature.buy_use_coupon.astype('float') / user2_feature.coupon_received.astype('float')\n",
    "user2_feature.buy_total = user2_feature.buy_total.replace(np.nan,0)\n",
    "user2_feature.coupon_received = user2_feature.coupon_received.replace(np.nan,0)\n",
    "user2_feature.to_csv('data1/user2_feature.csv',index=None)   \n",
    "\n",
    "\n",
    "\n",
    "#for dataset1\n",
    "\n",
    "user1 = feature1[['user_id', 'merchant_id', 'coupon_id', 'discount_rate','distance', 'date_received', 'date']]\n",
    "t = user1[['user_id']].copy()\n",
    "t.drop_duplicates(inplace = True)\n",
    "#用户消费商户数量\n",
    "t1 = user1[user1.date != 'null'][['user_id', 'merchant_id']].copy()\n",
    "t1.drop_duplicates(inplace = True)\n",
    "t1.merchant_id = 1\n",
    "t1 = t1.groupby('user_id').agg(sum).reset_index()\n",
    "t1.rename(columns = {'merchant_id': 'count_merchant'}, inplace = True)\n",
    "#用户消费次数\n",
    "t2 = user1[user1.date != 'null'][['user_id']].copy()\n",
    "t2['buy_total'] = 1\n",
    "t2 = t2.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#用户使用优惠券消费次数\n",
    "t3 = user1[(user1.date != 'null') & (user1.coupon_id!= 'null')][['user_id']].copy()\n",
    "t3['buy_use_coupon'] = 1\n",
    "t3 = t3.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#用户领取优惠券次数\n",
    "t4 = user1[user1.coupon_id != 'null'][['user_id']].copy()\n",
    "t4['coupon_received'] = 1\n",
    "t4 = t4.groupby('user_id').agg(sum).reset_index()\n",
    "\n",
    "#所有使用优惠券消费的商户与用户最大、最小、平均距离\n",
    "t5 = user1[(user1.coupon_id!= 'null') & (user1.date !='null')][['user_id', 'distance']].copy()\n",
    "t5.replace('null', -1, inplace = True)\n",
    "t5.distance = t5.distance.astype('int')\n",
    "t5.replace(-1, np.nan, inplace = True)\n",
    "t6 = t5.groupby('user_id').agg(min).reset_index()\n",
    "t6.rename(columns = {'distance':'user_min_distance'}, inplace = True)\n",
    "\n",
    "t7 = t5.groupby('user_id').agg(max).reset_index()\n",
    "t7.rename(columns = {'distance':'user_max_distance'}, inplace = True)\n",
    "\n",
    "t8 = t5.groupby('user_id').agg('mean').reset_index()\n",
    "t8.rename(columns = {'distance':'user_mean_distance'}, inplace = True)\n",
    "\n",
    "t9 = user1[(user1.date!='null')&(user1.date_received !='null')][['user_id', 'date_received', 'date']].copy()\n",
    "t9['user_date_datereceived_gap'] = t9.date + ':' +t9.date_received\n",
    "t9.user_date_datereceived_gap = t9.user_date_datereceived_gap.apply(get_user_date_datereceived_gap)\n",
    "t9 = t9[['user_id', 'user_date_datereceived_gap']].copy()\n",
    "\n",
    "t10 = t9.groupby('user_id').agg(min).reset_index()\n",
    "t10.rename(columns = {'user_date_datereceived_gap':'min_user_date_datereceived_gap'}, inplace=True)\n",
    "                                                                \n",
    "t11 = t9.groupby('user_id').agg(max).reset_index()\n",
    "t11.rename(columns = {'user_date_datereceived_gap':'max_user_date_datereceived_gap'}, inplace=True)\n",
    "t12 = t9.groupby('user_id').agg('mean').reset_index()\n",
    "t12.rename(columns = {'user_date_datereceived_gap':'avg_user_date_datereceived_gap'}, inplace=True)\n",
    "\n",
    "user1_feature = pd.merge(t,t1,on='user_id',how='left')                                                               \n",
    "user1_feature = pd.merge(user1_feature,t2,on='user_id',how='left') \n",
    "user1_feature = pd.merge(user1_feature,t3,on='user_id',how='left') \n",
    "user1_feature = pd.merge(user1_feature,t4,on='user_id',how='left') \n",
    "user1_feature = pd.merge(user1_feature,t6,on='user_id',how='left') \n",
    "user1_feature = pd.merge(user1_feature,t7,on='user_id',how='left') \n",
    "user1_feature = pd.merge(user1_feature,t8,on='user_id',how='left')  \n",
    "user1_feature = pd.merge(user1_feature,t10,on='user_id',how='left')    \n",
    "user1_feature = pd.merge(user1_feature,t11,on='user_id',how='left')        \n",
    "user1_feature = pd.merge(user1_feature,t12,on='user_id',how='left')  \n",
    "user1_feature.count_merchant = user1_feature.count_merchant.replace(np.nan,0)\n",
    "user1_feature.buy_use_coupon = user1_feature.buy_use_coupon.replace(np.nan,0)\n",
    "user1_feature['buy_use_coupon_rate'] = user1_feature.buy_use_coupon.astype('float') / user1_feature.buy_total.astype('float')\n",
    "user1_feature['user_coupon_transfer_rate'] = user1_feature.buy_use_coupon.astype('float') / user1_feature.coupon_received.astype('float')\n",
    "user1_feature.buy_total = user1_feature.buy_total.replace(np.nan,0)\n",
    "user1_feature.coupon_received = user1_feature.coupon_received.replace(np.nan,0)\n",
    "user1_feature.to_csv('data1/user1_feature.csv',index=None)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  user_merchant related feature #########################\n",
    "\n",
    "\"\"\"\n",
    "4.user_merchant:\n",
    "      user_merchant_buy_total.  user_merchant_received    user_merchant_buy_use_coupon  user_merchant_any  user_merchant_buy_common\n",
    "--       user_merchant_coupon_transform_rate = user_merchant_buy_use_coupon/user_merchant_received\n",
    "--       user_merchant_coupon_buy_rate = user_merchant_buy_use_coupon/user_merchant_buy_total\n",
    "--       user_merchant_common_buy_rate = user_merchant_buy_common/user_merchant_buy_total\n",
    "--       user_merchant_rate = user_merchant_buy_total/user_merchant_any \n",
    "\"\"\"\n",
    "#for dataset3\n",
    "all_user_merchant = feature3[['user_id','merchant_id']].copy()\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "#用户在商户消费次数\n",
    "t = feature3[['user_id', 'merchant_id', 'date']].copy()\n",
    "t = t[t.date!='null'][['user_id', 'merchant_id']].copy()\n",
    "t['user_merchant_buy_total'] =1\n",
    "t = t.groupby(['user_id','merchant_id']).agg(sum).reset_index()\n",
    "t.drop_duplicates(inplace = True)\n",
    "\n",
    "#用户在商户使用优惠券的消费次数\n",
    "t1 = feature3[['user_id', 'merchant_id','coupon_id']].copy()\n",
    "t1 = t1[t1.coupon_id != 'null'][['user_id','merchant_id']].copy()\n",
    "t1['user_merchant_received'] =1\n",
    "t1 = t1.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t1.drop_duplicates(inplace= True)\n",
    "\n",
    "#用户在商户使用优惠券的消费次数\n",
    "t2 = feature3[['user_id', 'merchant_id', 'coupon_id', 'date']].copy()\n",
    "t2 = t2[(t2.coupon_id!= 'null')&(t2.date!= 'null')][['user_id', 'merchant_id']].copy()\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t2.drop_duplicates(inplace= True)\n",
    "\n",
    "#用户和商户的数量\n",
    "t3 = feature3[['user_id', 'merchant_id']].copy()\n",
    "t3['user_merchant_any']=1\n",
    "t3 = t3.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "#用户对商户普通消费次数\n",
    "t4 = feature3[['user_id', 'merchant_id','coupon_id', 'date']].copy()\n",
    "t4 = t4[(t4.date!= 'null')&(t4.coupon_id == 'null')].copy()\n",
    "t4['user_merchant_buy_common'] =1\n",
    "t4 = t4.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t4.drop_duplicates(inplace = True)\n",
    "\n",
    "user_merchant3 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3 = pd.merge(user_merchant3,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant3.user_merchant_buy_use_coupon = user_merchant3.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant3.user_merchant_buy_common = user_merchant3.user_merchant_buy_common.replace(np.nan,0)\n",
    "#用户对商户的优惠券转化率\n",
    "user_merchant3['user_merchant_coupon_transfer_rate'] = user_merchant3.user_merchant_buy_use_coupon.astype('float') / user_merchant3.user_merchant_received.astype('float')\n",
    "#用户对商户使用优惠券消费占总消费比例\n",
    "user_merchant3['user_merchant_coupon_buy_rate'] = user_merchant3.user_merchant_buy_use_coupon.astype('float') / user_merchant3.user_merchant_buy_total.astype('float')\n",
    "#用户对商户消费占总交互比例\n",
    "user_merchant3['user_merchant_rate'] = user_merchant3.user_merchant_buy_total.astype('float') / user_merchant3.user_merchant_any.astype('float')\n",
    "#用户对商户使用优惠券占总消费比例\n",
    "user_merchant3['user_merchant_common_buy_rate'] = user_merchant3.user_merchant_buy_common.astype('float') / user_merchant3.user_merchant_buy_total.astype('float')\n",
    "user_merchant3.to_csv('data1/user_merchant3.csv',index=None)\n",
    "\n",
    "\n",
    "#for dataset2\n",
    "all_user_merchant = feature2[['user_id','merchant_id']].copy()\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "#用户在商户消费次数\n",
    "t = feature2[['user_id', 'merchant_id', 'date']].copy()\n",
    "t = t[t.date!='null'][['user_id', 'merchant_id']].copy()\n",
    "t['user_merchant_buy_total'] =1\n",
    "t = t.groupby(['user_id','merchant_id']).agg(sum).reset_index()\n",
    "t.drop_duplicates(inplace = True)\n",
    "\n",
    "#用户在商户使用优惠券的消费次数\n",
    "t1 = feature2[['user_id', 'merchant_id','coupon_id']].copy()\n",
    "t1 = t1[t1.coupon_id != 'null'][['user_id','merchant_id']].copy()\n",
    "t1['user_merchant_received'] =1\n",
    "t1 = t1.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t1.drop_duplicates(inplace= True)\n",
    "\n",
    "#用户在商户使用优惠券的消费次数\n",
    "t2 = feature2[['user_id', 'merchant_id', 'coupon_id', 'date']].copy()\n",
    "t2 = t2[(t2.coupon_id!= 'null')&(t2.date!= 'null')][['user_id', 'merchant_id']].copy()\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t2.drop_duplicates(inplace= True)\n",
    "\n",
    "#用户和商户的数量\n",
    "t3 = feature2[['user_id', 'merchant_id']].copy()\n",
    "t3['user_merchant_any']=1\n",
    "t3 = t3.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "#用户对商户普通消费次数\n",
    "t4 = feature2[['user_id', 'merchant_id','coupon_id', 'date']].copy()\n",
    "t4 = t4[(t4.date!= 'null')&(t4.coupon_id == 'null')].copy()\n",
    "t4['user_merchant_buy_common'] =1\n",
    "t4 = t4.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t4.drop_duplicates(inplace = True)\n",
    "\n",
    "user_merchant2 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2 = pd.merge(user_merchant2,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant2.user_merchant_buy_use_coupon = user_merchant2.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant2.user_merchant_buy_common = user_merchant2.user_merchant_buy_common.replace(np.nan,0)\n",
    "#用户对商户的优惠券转化率\n",
    "user_merchant2['user_merchant_coupon_transfer_rate'] = user_merchant2.user_merchant_buy_use_coupon.astype('float') / user_merchant2.user_merchant_received.astype('float')\n",
    "#用户对商户使用优惠券消费占总消费比例\n",
    "user_merchant2['user_merchant_coupon_buy_rate'] = user_merchant2.user_merchant_buy_use_coupon.astype('float') / user_merchant2.user_merchant_buy_total.astype('float')\n",
    "#用户对商户消费占总交互比例\n",
    "user_merchant2['user_merchant_rate'] = user_merchant2.user_merchant_buy_total.astype('float') / user_merchant2.user_merchant_any.astype('float')\n",
    "#用户对商户使用优惠券占总消费比例\n",
    "user_merchant2['user_merchant_common_buy_rate'] = user_merchant2.user_merchant_buy_common.astype('float') / user_merchant2.user_merchant_buy_total.astype('float')\n",
    "user_merchant2.to_csv('data1/user_merchant2.csv',index=None)\n",
    "\n",
    "\n",
    "\n",
    "#for dataset1\n",
    "all_user_merchant = feature1[['user_id','merchant_id']].copy()\n",
    "all_user_merchant.drop_duplicates(inplace=True)\n",
    "\n",
    "#用户在商户消费次数\n",
    "t = feature1[['user_id', 'merchant_id', 'date']].copy()\n",
    "t = t[t.date!='null'][['user_id', 'merchant_id']].copy()\n",
    "t['user_merchant_buy_total'] =1\n",
    "t = t.groupby(['user_id','merchant_id']).agg(sum).reset_index()\n",
    "t.drop_duplicates(inplace = True)\n",
    "\n",
    "#用户在商户使用优惠券的消费次数\n",
    "t1 = feature1[['user_id', 'merchant_id','coupon_id']].copy()\n",
    "t1 = t1[t1.coupon_id != 'null'][['user_id','merchant_id']].copy()\n",
    "t1['user_merchant_received'] =1\n",
    "t1 = t1.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t1.drop_duplicates(inplace= True)\n",
    "\n",
    "#用户在商户使用优惠券的消费次数\n",
    "t2 = feature1[['user_id', 'merchant_id', 'coupon_id', 'date']].copy()\n",
    "t2 = t2[(t2.coupon_id!= 'null')&(t2.date!= 'null')][['user_id', 'merchant_id']].copy()\n",
    "t2['user_merchant_buy_use_coupon'] = 1\n",
    "t2 = t2.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t2.drop_duplicates(inplace= True)\n",
    "\n",
    "#用户和商户的数量\n",
    "t3 = feature1[['user_id', 'merchant_id']].copy()\n",
    "t3['user_merchant_any']=1\n",
    "t3 = t3.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t3.drop_duplicates(inplace=True)\n",
    "\n",
    "#用户对商户普通消费次数\n",
    "t4 = feature1[['user_id', 'merchant_id','coupon_id', 'date']].copy()\n",
    "t4 = t4[(t4.date!= 'null')&(t4.coupon_id == 'null')].copy()\n",
    "t4['user_merchant_buy_common'] =1\n",
    "t4 = t4.groupby(['user_id', 'merchant_id']).agg(sum).reset_index()\n",
    "t4.drop_duplicates(inplace = True)\n",
    "\n",
    "user_merchant1 = pd.merge(all_user_merchant,t,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t1,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t2,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t3,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1 = pd.merge(user_merchant1,t4,on=['user_id','merchant_id'],how='left')\n",
    "user_merchant1.user_merchant_buy_use_coupon = user_merchant1.user_merchant_buy_use_coupon.replace(np.nan,0)\n",
    "user_merchant1.user_merchant_buy_common = user_merchant1.user_merchant_buy_common.replace(np.nan,0)\n",
    "#用户对商户的优惠券转化率\n",
    "user_merchant1['user_merchant_coupon_transfer_rate'] = user_merchant1.user_merchant_buy_use_coupon.astype('float') / user_merchant1.user_merchant_received.astype('float')\n",
    "#用户对商户使用优惠券消费占总消费比例\n",
    "user_merchant1['user_merchant_coupon_buy_rate'] = user_merchant1.user_merchant_buy_use_coupon.astype('float') / user_merchant1.user_merchant_buy_total.astype('float')\n",
    "#用户对商户消费占总交互比例\n",
    "user_merchant1['user_merchant_rate'] = user_merchant1.user_merchant_buy_total.astype('float') / user_merchant1.user_merchant_any.astype('float')\n",
    "#用户对商户使用优惠券占总消费比例\n",
    "user_merchant1['user_merchant_common_buy_rate'] = user_merchant1.user_merchant_buy_common.astype('float') / user_merchant1.user_merchant_buy_total.astype('float')\n",
    "user_merchant1.to_csv('data1/user_merchant1.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "user_coupon:\n",
    "对label窗里的user_coupon，特征窗里用户领取过该coupon几次   label_user_coupon_feature_receive_count\n",
    "对label窗里的user_coupon，特征窗里用户用该coupon消费过几次   label_user_coupon_feature_buy_count\n",
    "对label窗里的user_coupon，特征窗里用户对该coupon的核销率   label_user_coupon_feature_rate = label_user_coupon_feature_buy_count/label_user_coupon_feature_receive_count\n",
    "\"\"\"\n",
    "def get_label_user_coupon_feature_receive_count(s, df):\n",
    "    s = s.split(':')\n",
    "    if (s[0],s[1]) in df.index:\n",
    "        return df.loc[(s[0],s[1])]    \n",
    "    else:\n",
    "        return 0 \n",
    "#for dataset3\n",
    "user_coupon3 = dataset3[['user_id', 'coupon_id']].copy()\n",
    "#user_coupon3 = user_coupon3[user_coupon3.coupon_id != 'null'].copy()\n",
    "user_coupon3.drop_duplicates(inplace = True)\n",
    "\n",
    "t = feature3[['date_received', 'user_id', 'coupon_id']].copy()\n",
    "t = t[t.date_received != 'null'][['user_id', 'coupon_id']].copy()\n",
    "t['label_user_coupon_feature_receive_count'] = 1\n",
    "t = t.groupby(['user_id', 'coupon_id']).agg(sum)\n",
    "\n",
    "t1 = feature3[['user_id', 'coupon_id', 'date']].copy()\n",
    "t1 = t1[t1.date != 'null'][['user_id', 'coupon_id']].copy()\n",
    "t1['label_user_coupon_feature_buy_count'] = 1\n",
    "t1 = t1.groupby(['user_id', 'coupon_id']).agg(sum)\n",
    "print ('------')\n",
    "\n",
    "user_coupon3['user_coupon'] = user_coupon3.user_id.astype('str')+ ':'+user_coupon3.coupon_id.astype('str')\n",
    "user_coupon3['label_user_coupon_feature_receive_count'] = user_coupon3.user_coupon.apply(get_label_user_coupon_feature_receive_count, df=t)\n",
    "user_coupon3['label_user_coupon_feature_buy_count'] = user_coupon3.user_coupon.apply(get_label_user_coupon_feature_receive_count, df=t1)\n",
    "user_coupon3['label_user_coupon_feature_rate'] = user_coupon3.label_user_coupon_feature_buy_count.astype('float')/user_coupon3.label_user_coupon_feature_receive_count.astype('float')\n",
    "user_coupon3.drop('user_coupon',axis=1, inplace=True)\n",
    "user_coupon3.to_csv('data1/user_coupon3.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset2\n",
    "user_coupon2 = dataset2[['user_id', 'coupon_id']].copy()\n",
    "user_coupon2 = user_coupon2[user_coupon2.coupon_id != 'null'].copy()\n",
    "user_coupon2.drop_duplicates(inplace = True)\n",
    "\n",
    "t = feature2[['date_received', 'user_id', 'coupon_id']].copy()\n",
    "t = t[t.date_received != 'null'][['user_id', 'coupon_id']].copy()\n",
    "t['label_user_coupon_feature_receive_count'] = 1\n",
    "t = t.groupby(['user_id', 'coupon_id']).agg(sum)\n",
    "\n",
    "t1 = feature2[['user_id', 'coupon_id', 'date']].copy()\n",
    "t1 = t1[t1.date != 'null'][['user_id', 'coupon_id']].copy()\n",
    "t1['label_user_coupon_feature_buy_count'] = 1\n",
    "t1 = t1.groupby(['user_id', 'coupon_id']).agg(sum)\n",
    "\n",
    "user_coupon2['user_coupon'] = user_coupon2.user_id.astype('str')+ ':'+user_coupon2.coupon_id.astype('str')\n",
    "user_coupon2['label_user_coupon_feature_receive_count'] = user_coupon2.user_coupon.apply(get_label_user_coupon_feature_receive_count, df=t)\n",
    "user_coupon2['label_user_coupon_feature_buy_count'] = user_coupon2.user_coupon.apply(get_label_user_coupon_feature_receive_count, df=t1)\n",
    "user_coupon2['label_user_coupon_feature_rate'] = user_coupon2.label_user_coupon_feature_buy_count.astype('float')/user_coupon2.label_user_coupon_feature_receive_count.astype('float')\n",
    "user_coupon2.drop('user_coupon',axis=1, inplace=True)\n",
    "user_coupon2.to_csv('data1/user_coupon2.csv',index=None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset1\n",
    "user_coupon1 = dataset1[['user_id', 'coupon_id']].copy()\n",
    "user_coupon1 = user_coupon1[user_coupon1.coupon_id != 'null'].copy()\n",
    "user_coupon1.drop_duplicates(inplace = True)\n",
    "\n",
    "t = feature1[['date_received', 'user_id', 'coupon_id']].copy()\n",
    "t = t[t.date_received != 'null'][['user_id', 'coupon_id']].copy()\n",
    "t['label_user_coupon_feature_receive_count'] = 1\n",
    "t = t.groupby(['user_id', 'coupon_id']).agg(sum)\n",
    "\n",
    "t1 = feature1[['user_id', 'coupon_id', 'date']].copy()\n",
    "t1 = t1[t1.date != 'null'][['user_id', 'coupon_id']].copy()\n",
    "t1['label_user_coupon_feature_buy_count'] = 1\n",
    "t1 = t1.groupby(['user_id', 'coupon_id']).agg(sum)\n",
    "\n",
    "user_coupon1['user_coupon'] = user_coupon1.user_id.astype('str')+ ':'+user_coupon1.coupon_id.astype('str')\n",
    "user_coupon1['label_user_coupon_feature_receive_count'] = user_coupon1.user_coupon.apply(get_label_user_coupon_feature_receive_count, df=t)\n",
    "user_coupon1['label_user_coupon_feature_buy_count'] = user_coupon1.user_coupon.apply(get_label_user_coupon_feature_receive_count, df=t1)\n",
    "user_coupon1['label_user_coupon_feature_rate'] = user_coupon1.label_user_coupon_feature_buy_count.astype('float')/user_coupon1.label_user_coupon_feature_receive_count.astype('float')\n",
    "user_coupon1.drop('user_coupon',axis=1, inplace=True)\n",
    "user_coupon1.to_csv('data1/user_coupon1.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "其他特征：\n",
    "增加特征：商家有交集的用户数目 label_merchant_user_count\n",
    "--       商家发出的所有优惠券数目  label_merchant_coupon_count\n",
    "--       商家发出的所有优惠券种类数目  label_merchant_coupon_type_count\n",
    "--       用户领取该商家的所有优惠券数目  label_user_merchant_coupon_count\n",
    "--       用户在此次优惠券之后还领取了多少该优惠券   label_same_coupon_count_later\n",
    "--       用户在此次优惠券之后还领取了多少优惠券     label_coupon_count_later\n",
    "--       用户有交集的商家数目     label_user_merchant_count\n",
    "'''\n",
    "t8 = dataset3[['merchant_id','user_id']].copy()\n",
    "t8.drop_duplicates(inplace = True)\n",
    "t8['label_merchant_user_count'] = 1\n",
    "t8 = t8.groupby('merchant_id').agg(sum).reset_index()\n",
    "t8 = t8[['merchant_id', 'label_merchant_user_count']].copy()\n",
    "\n",
    "t9 = dataset3[['merchant_id','coupon_id']].copy()\n",
    "#t9 = t9[t9.coupon_id!= 'null'].copy()\n",
    "t9.drop_duplicates(inplace = True)\n",
    "t9['label_merchant_coupon_type_count'] = 1\n",
    "t9 = t9.groupby('merchant_id').agg(sum).reset_index()\n",
    "t9 = t9[['merchant_id', 'label_merchant_coupon_type_count']].copy()\n",
    "\n",
    "t10 = dataset3[['merchant_id','coupon_id']].copy()\n",
    "#t10 = t10[t10.coupon_id!= 'null'].copy()\n",
    "t10['label_merchant_coupon_count'] = 1\n",
    "t10 = t10.groupby('merchant_id').agg(sum).reset_index()\n",
    "t10 = t10[['merchant_id', 'label_merchant_coupon_count']].copy() \n",
    "\n",
    "t11 = dataset3[['user_id','merchant_id','coupon_id']].copy()\n",
    "#t11 = t11[t11.coupon_id!= 'null'].copy()\n",
    "t11['label_user_merchant_coupon_count'] = 1\n",
    "t11 = t11.groupby(['merchant_id','user_id']).agg(sum).reset_index()\n",
    "t11 = t11[['merchant_id', 'user_id','label_user_merchant_coupon_count']].copy() \n",
    " \n",
    "t12 = dataset3[['merchant_id','user_id']].copy()\n",
    "t12.drop_duplicates(inplace = True)\n",
    "t12['label_user_merchant_count'] =1\n",
    "t12 = t12.groupby('user_id').agg(sum).reset_index()\n",
    "t12 = t12[['user_id', 'label_user_merchant_count']].copy()\n",
    "\n",
    "\n",
    "\n",
    "#for dataset3\n",
    "t = dataset3[['user_id']].copy()\n",
    "#t.loc[: , 'this_month_user_receive_all_coupon_count']=1\n",
    "t['this_month_user_receive_all_coupon_count'] = 1#相当于给原有数据加上一列，这个月用户收取的所有优惠券数目，并初始化为1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "#将t按照用户id进行分组，然后统计所有用户收取的优惠券数目,并初始化一个索引值\n",
    "t1 = dataset3[['user_id','coupon_id']].copy()#提取数据集3的优惠券Id和用户Id\n",
    "#t1.loc[:, 'this_month_user_receive_same_coupon_count']=1\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "t2 = dataset3[['user_id','coupon_id','date_received']].copy()\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([int(d) for d in s.split(':')]))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([int(d) for d in s.split(':')]))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "t3 = dataset3[['user_id','coupon_id','date_received']].copy()\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')#将两表融合只保留左表数据,这样得到的表，相当于保留了最近接收时间和最远接受时间\n",
    "\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset3[['user_id','date_received']].copy()\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset3[['user_id','coupon_id','date_received']].copy()\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t6 = dataset3[['user_id','coupon_id','date_received']].copy()\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n",
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "    \n",
    "\n",
    "t7 = dataset3[['user_id','coupon_id','date_received']].copy()\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)#用户上/下一次领取的时间间隔\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']].copy()\n",
    "\n",
    "other_feature3 = pd.merge(t1,t,on='user_id')\n",
    "other_feature3 = pd.merge(other_feature3,t3,on=['user_id','coupon_id'])\n",
    "other_feature3 = pd.merge(other_feature3,t4,on=['user_id','date_received'])\n",
    "other_feature3 = pd.merge(other_feature3,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature3 = pd.merge(other_feature3,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature3 = pd.merge(other_feature3,t12,on=['user_id'])\n",
    "other_feature3 = pd.merge(other_feature3,t11,on=['user_id'])\n",
    "other_feature3 = pd.merge(other_feature3,t10,on=['merchant_id'])\n",
    "other_feature3 = pd.merge(other_feature3,t9,on=['merchant_id'])\n",
    "other_feature3 = pd.merge(other_feature3,t8,on=['merchant_id'])\n",
    "other_feature3.to_csv('data1/other_feature3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset2\n",
    "t8 = dataset2[['merchant_id','user_id']].copy()\n",
    "t8.drop_duplicates(inplace = True)\n",
    "t8['label_merchant_user_count'] = 1\n",
    "t8 = t8.groupby('merchant_id').agg(sum).reset_index()\n",
    "t8 = t8[['merchant_id', 'label_merchant_user_count']].copy()\n",
    "\n",
    "t9 = dataset2[['merchant_id','coupon_id']].copy()\n",
    "t9 = t9[t9.coupon_id!= 'null'].copy()\n",
    "t9.drop_duplicates(inplace = True)\n",
    "t9['label_merchant_coupon_type_count'] = 1\n",
    "t9 = t9.groupby('merchant_id').agg(sum).reset_index()\n",
    "t9 = t9[['merchant_id', 'label_merchant_coupon_type_count']].copy()\n",
    "\n",
    "t10 = dataset2[['merchant_id','coupon_id']].copy()\n",
    "t10 = t10[t10.coupon_id!= 'null'].copy()\n",
    "t10['label_merchant_coupon_count'] = 1\n",
    "t10 = t10.groupby('merchant_id').agg(sum).reset_index()\n",
    "t10 = t10[['merchant_id', 'label_merchant_coupon_count']].copy() \n",
    "\n",
    "t11 = dataset2[['user_id','merchant_id','coupon_id']].copy()\n",
    "t11 = t11[t11.coupon_id!= 'null'].copy()\n",
    "t11['label_user_merchant_coupon_count'] = 1\n",
    "t11 = t11.groupby(['merchant_id','user_id']).agg(sum).reset_index()\n",
    "t11 = t11[['merchant_id', 'user_id','label_user_merchant_coupon_count']].copy() \n",
    " \n",
    "t12 = dataset2[['merchant_id','user_id']].copy()\n",
    "t12.drop_duplicates(inplace = True)\n",
    "t12['label_user_merchant_count'] =1\n",
    "t12 = t12.groupby('user_id').agg(sum).reset_index()\n",
    "t12 = t12[['user_id', 'label_user_merchant_count']].copy()\n",
    "\n",
    "t = dataset2[['user_id']].copy()\n",
    "t['this_month_user_receive_all_coupon_count'] = 1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t1 = dataset2[['user_id','coupon_id']].copy()\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "\n",
    "t2 = dataset2[['user_id','coupon_id','date_received']].copy()\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([int(d) for d in s.split(':')]))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([int(d) for d in s.split(':')]))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "\n",
    "t3 = dataset2[['user_id','coupon_id','date_received']].copy()\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received.astype('int')\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received.astype('int') - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset2[['user_id','date_received']].copy()\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset2[['user_id','coupon_id','date_received']].copy()\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t6 = dataset2[['user_id','coupon_id','date_received']].copy()\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n",
    "\n",
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "    \n",
    "\n",
    "t7 = dataset2[['user_id','coupon_id','date_received']].copy()\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']].copy()\n",
    "\n",
    "other_feature2 = pd.merge(t1,t,on='user_id')\n",
    "other_feature2 = pd.merge(other_feature2,t3,on=['user_id','coupon_id'])\n",
    "other_feature2 = pd.merge(other_feature2,t4,on=['user_id','date_received'])\n",
    "other_feature2 = pd.merge(other_feature2,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature2 = pd.merge(other_feature2,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature2 = pd.merge(other_feature2,t12,on=['user_id'])\n",
    "other_feature2 = pd.merge(other_feature2,t11,on=['user_id'])\n",
    "other_feature2 = pd.merge(other_feature2,t10,on=['merchant_id'])\n",
    "other_feature2 = pd.merge(other_feature2,t9,on=['merchant_id'])\n",
    "other_feature2 = pd.merge(other_feature2,t8,on=['merchant_id'])\n",
    "other_feature2.to_csv('data1/other_feature2.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset1\n",
    "t8 = dataset1[['merchant_id','user_id']].copy()\n",
    "t8.drop_duplicates(inplace = True)\n",
    "t8['label_merchant_user_count'] = 1\n",
    "t8 = t8.groupby('merchant_id').agg(sum).reset_index()\n",
    "t8 = t8[['merchant_id', 'label_merchant_user_count']].copy()\n",
    "\n",
    "t9 = dataset1[['merchant_id','coupon_id']].copy()\n",
    "t9 = t9[t9.coupon_id!= 'null'].copy()\n",
    "t9.drop_duplicates(inplace = True)\n",
    "t9['label_merchant_coupon_type_count'] = 1\n",
    "t9 = t9.groupby('merchant_id').agg(sum).reset_index()\n",
    "t9 = t9[['merchant_id', 'label_merchant_coupon_type_count']].copy()\n",
    "\n",
    "t10 = dataset1[['merchant_id','coupon_id']].copy()\n",
    "t10 = t10[t10.coupon_id!= 'null'].copy()\n",
    "t10['label_merchant_coupon_count'] = 1\n",
    "t10 = t10.groupby('merchant_id').agg(sum).reset_index()\n",
    "t10 = t10[['merchant_id', 'label_merchant_coupon_count']].copy() \n",
    "\n",
    "t11 = dataset1[['user_id','merchant_id','coupon_id']].copy()\n",
    "t11 = t11[t11.coupon_id!= 'null'].copy()\n",
    "t11['label_user_merchant_coupon_count'] = 1\n",
    "t11 = t11.groupby(['merchant_id','user_id']).agg(sum).reset_index()\n",
    "t11 = t11[['merchant_id', 'user_id','label_user_merchant_coupon_count']].copy() \n",
    " \n",
    "t12 = dataset1[['merchant_id','user_id']].copy()\n",
    "t12.drop_duplicates(inplace = True)\n",
    "t12['label_user_merchant_count'] =1\n",
    "t12 = t12.groupby('user_id').agg(sum).reset_index()\n",
    "t12 = t12[['user_id', 'label_user_merchant_count']].copy()\n",
    "\n",
    "t = dataset1[['user_id']].copy()\n",
    "t['this_month_user_receive_all_coupon_count'] = 1\n",
    "t = t.groupby('user_id').agg('sum').reset_index()\n",
    "\n",
    "t1 = dataset1[['user_id','coupon_id']].copy()\n",
    "t1['this_month_user_receive_same_coupon_count'] = 1\n",
    "t1 = t1.groupby(['user_id','coupon_id']).agg('sum').reset_index()\n",
    "\n",
    "t2 = dataset1[['user_id','coupon_id','date_received']].copy()\n",
    "t2.date_received = t2.date_received.astype('str')\n",
    "t2 = t2.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t2['receive_number'] = t2.date_received.apply(lambda s:len(s.split(':')))\n",
    "t2 = t2[t2.receive_number>1]\n",
    "t2['max_date_received'] = t2.date_received.apply(lambda s:max([int(d) for d in s.split(':')]))\n",
    "t2['min_date_received'] = t2.date_received.apply(lambda s:min([int(d) for d in s.split(':')]))\n",
    "t2 = t2[['user_id','coupon_id','max_date_received','min_date_received']]\n",
    "\n",
    "t3 = dataset1[['user_id','coupon_id','date_received']].copy()\n",
    "t3 = pd.merge(t3,t2,on=['user_id','coupon_id'],how='left')\n",
    "t3['this_month_user_receive_same_coupon_lastone'] = t3.max_date_received - t3.date_received.astype('int')\n",
    "t3['this_month_user_receive_same_coupon_firstone'] = t3.date_received.astype('int') - t3.min_date_received\n",
    "def is_firstlastone(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    elif x>0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 #those only receive once\n",
    "        \n",
    "t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)\n",
    "t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)\n",
    "t3 = t3[['user_id','coupon_id','date_received','this_month_user_receive_same_coupon_lastone','this_month_user_receive_same_coupon_firstone']]\n",
    "\n",
    "t4 = dataset1[['user_id','date_received']].copy()\n",
    "t4['this_day_user_receive_all_coupon_count'] = 1\n",
    "t4 = t4.groupby(['user_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t5 = dataset1[['user_id','coupon_id','date_received']].copy()\n",
    "t5['this_day_user_receive_same_coupon_count'] = 1\n",
    "t5 = t5.groupby(['user_id','coupon_id','date_received']).agg('sum').reset_index()\n",
    "\n",
    "t6 = dataset1[['user_id','coupon_id','date_received']].copy()\n",
    "t6.date_received = t6.date_received.astype('str')\n",
    "t6 = t6.groupby(['user_id','coupon_id'])['date_received'].agg(lambda x:':'.join(x)).reset_index()\n",
    "t6.rename(columns={'date_received':'dates'},inplace=True)\n",
    "\n",
    "def get_day_gap_before(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))-date(int(d[0:4]),int(d[4:6]),int(d[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "        \n",
    "def get_day_gap_after(s):\n",
    "    date_received,dates = s.split('-')\n",
    "    dates = dates.split(':')\n",
    "    gaps = []\n",
    "    for d in dates:\n",
    "        this_gap = (date(int(d[0:4]),int(d[4:6]),int(d[6:8]))-date(int(date_received[0:4]),int(date_received[4:6]),int(date_received[6:8]))).days\n",
    "        if this_gap>0:\n",
    "            gaps.append(this_gap)\n",
    "    if len(gaps)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return min(gaps)\n",
    "    \n",
    "\n",
    "t7 = dataset1[['user_id','coupon_id','date_received']].copy()\n",
    "t7 = pd.merge(t7,t6,on=['user_id','coupon_id'],how='left')\n",
    "t7['date_received_date'] = t7.date_received.astype('str') + '-' + t7.dates\n",
    "t7['day_gap_before'] = t7.date_received_date.apply(get_day_gap_before)\n",
    "t7['day_gap_after'] = t7.date_received_date.apply(get_day_gap_after)\n",
    "t7 = t7[['user_id','coupon_id','date_received','day_gap_before','day_gap_after']].copy()\n",
    "\n",
    "other_feature1 = pd.merge(t1,t,on='user_id')\n",
    "other_feature1 = pd.merge(other_feature1,t3,on=['user_id','coupon_id'])\n",
    "other_feature1 = pd.merge(other_feature1,t4,on=['user_id','date_received'])\n",
    "other_feature1 = pd.merge(other_feature1,t5,on=['user_id','coupon_id','date_received'])\n",
    "other_feature1 = pd.merge(other_feature1,t7,on=['user_id','coupon_id','date_received'])\n",
    "other_feature1 = pd.merge(other_feature1,t12,on=['user_id'])\n",
    "other_feature1 = pd.merge(other_feature1,t11,on=['user_id'])\n",
    "other_feature1 = pd.merge(other_feature1,t10,on=['merchant_id'])\n",
    "other_feature1 = pd.merge(other_feature1,t9,on=['merchant_id'])\n",
    "other_feature1 = pd.merge(other_feature1,t8,on=['merchant_id'])\n",
    "other_feature1.to_csv('data1/other_feature1.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-73acbb9bc628>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m        \u001b[0monline_coupon_transform_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monline_buy_use_coupon\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0monline_coupon_received\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m '''\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mon_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/ccf_online_stage1_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mon_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'merchant_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'action'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'coupon_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'discount_rate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date_received'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1748\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1749\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas\\_libs\\parsers.c:10862)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas\\_libs\\parsers.c:11563)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._concatenate_chunks (pandas\\_libs\\parsers.c:29286)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "线上表的特征提取：\n",
    "7. online表的特征（都是用户相关）\n",
    "       用户线上购买总次数  online_buy_total\n",
    "       用户线上用coupon购买的总次数 online_buy_use_coupon\n",
    "       用户线上用fixed购买的总次数  online_buy_use_fixed\n",
    "       用户线上收到的coupon总次数   online_coupon_received\n",
    "       用户线上有发生购买的merchant个数  online_buy_merchant_count\n",
    "       用户线上有action的merchant个数      online_action_merchant_count\n",
    "       online_buy_use_coupon_fixed = online_buy_use_coupon+online_buy_use_fixed\n",
    "       online_buy_use_coupon_rate = online_buy_use_coupon/online_buy_total\n",
    "       online_buy_use_fixed_rate = online_buy_use_fixed/online_buy_total \n",
    "       online_buy_use_coupon_fixed_rate = online_buy_use_coupon_fixed/online_buy_total \n",
    "       online_coupon_transform_rate = online_buy_use_coupon/online_coupon_received\n",
    "'''\n",
    "on_train = pd.read_csv('data/ccf_online_stage1_train.csv',header=None)\n",
    "on_train.columns = ['user_id','merchant_id','action','coupon_id','discount_rate','date_received','date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112803, 63)\n",
      "(112803, 67)\n"
     ]
    }
   ],
   "source": [
    "##################  generate training and testing set ################\n",
    "def get_label(s):\n",
    "    s = s.split(':')\n",
    "    if s[0]=='null':\n",
    "        return 0\n",
    "    elif (date(int(s[0][0:4]),int(s[0][4:6]),int(s[0][6:8]))-date(int(s[1][0:4]),int(s[1][4:6]),int(s[1][6:8]))).days<=15:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "user_coupon3 = pd.read_csv('data1/user_coupon3.csv')\n",
    "coupon3 = pd.read_csv('data1/coupon3_feature.csv')\n",
    "merchant3 = pd.read_csv('data1/merchant3_feature.csv')\n",
    "user3 = pd.read_csv('data1/user3_feature.csv')\n",
    "user_merchant3 = pd.read_csv('data1/user_merchant3.csv')\n",
    "other_feature3 = pd.read_csv('data1/other_feature3.csv')\n",
    "dataset3 = pd.merge(coupon3,merchant3,on='merchant_id',how='left')\n",
    "dataset3 = pd.merge(dataset3,user3,on='user_id',how='left')\n",
    "dataset3 = pd.merge(dataset3,user_merchant3,on=['user_id','merchant_id'],how='left')\n",
    "dataset3 = pd.merge(dataset3,user_coupon3,on=['user_id','coupon_id'],how='left')\n",
    "dataset3 = pd.merge(dataset3,other_feature3,on=['user_id','coupon_id','date_received','merchant_id'],how='left')\n",
    "dataset3.drop_duplicates(inplace=True)\n",
    "print (dataset3.shape)\n",
    "\n",
    "dataset3.user_merchant_buy_total = dataset3.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset3.user_merchant_any = dataset3.user_merchant_any.replace(np.nan,0)\n",
    "dataset3.user_merchant_received = dataset3.user_merchant_received.replace(np.nan,0)\n",
    "\n",
    "weekday_dummies = pd.get_dummies(dataset3.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset3 = pd.concat([dataset3,weekday_dummies],axis=1)\n",
    "dataset3.drop(['merchant_id','day_of_week','coupon_count'],axis=1,inplace=True)\n",
    "#dataset3.drop(['merchant_id','day_of_week','date_received','coupon_id','coupon_count'],axis=1,inplace=True)\n",
    "dataset3 = dataset3.replace('null',np.nan)\n",
    "dataset3.to_csv('data1/dataset3.csv',index=None)\n",
    "print (dataset3.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257126, 64)\n",
      "(257126, 68)\n"
     ]
    }
   ],
   "source": [
    "user_coupon2 = pd.read_csv('data1/user_coupon2.csv')\n",
    "coupon2 = pd.read_csv('data1/coupon2_feature.csv')\n",
    "merchant2 = pd.read_csv('data1/merchant2_feature.csv')\n",
    "user2 = pd.read_csv('data1/user2_feature.csv')\n",
    "user_merchant2 = pd.read_csv('data1/user_merchant2.csv')\n",
    "other_feature2 = pd.read_csv('data1/other_feature2.csv')\n",
    "dataset2 = pd.merge(coupon2,merchant2,on='merchant_id',how='left')\n",
    "dataset2 = pd.merge(dataset2,user2,on='user_id',how='left')\n",
    "dataset2 = pd.merge(dataset2,user_merchant2,on=['user_id','merchant_id'],how='left')\n",
    "dataset2 = pd.merge(dataset2,user_coupon2,on=['user_id','coupon_id'],how='left')\n",
    "dataset2 = pd.merge(dataset2,other_feature2,on=['user_id','coupon_id','date_received','merchant_id'],how='left')\n",
    "dataset2.drop_duplicates(inplace=True)\n",
    "print (dataset2.shape)\n",
    "\n",
    "dataset2.user_merchant_buy_total = dataset2.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset2.user_merchant_any = dataset2.user_merchant_any.replace(np.nan,0)\n",
    "dataset2.user_merchant_received = dataset2.user_merchant_received.replace(np.nan,0)\n",
    "\n",
    "weekday_dummies = pd.get_dummies(dataset2.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset2 = pd.concat([dataset2,weekday_dummies],axis=1)\n",
    "dataset2['label'] = dataset2.date.astype('str') + ':' +  dataset2.date_received.astype('str')\n",
    "dataset2.label = dataset2.label.apply(get_label)\n",
    "dataset2.drop(['merchant_id','day_of_week','date','coupon_count'],axis=1,inplace=True)\n",
    "dataset2 = dataset2.replace('null',np.nan)\n",
    "dataset2.to_csv('data1/dataset2.csv',index=None)\n",
    "print (dataset2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136301, 64)\n",
      "(136301, 68)\n"
     ]
    }
   ],
   "source": [
    "user_coupon1 = pd.read_csv('data1/user_coupon1.csv')\n",
    "coupon1 = pd.read_csv('data1/coupon1_feature.csv')\n",
    "merchant1 = pd.read_csv('data1/merchant1_feature.csv')\n",
    "user1 = pd.read_csv('data1/user1_feature.csv')\n",
    "user_merchant1 = pd.read_csv('data/user_merchant1.csv')\n",
    "other_feature1 = pd.read_csv('data/other_feature1.csv')\n",
    "dataset1 = pd.merge(coupon1,merchant1,on='merchant_id',how='left')\n",
    "dataset1 = pd.merge(dataset1,user1,on='user_id',how='left')\n",
    "dataset1 = pd.merge(dataset1,user_merchant1,on=['user_id','merchant_id'],how='left')\n",
    "dataset1 = pd.merge(dataset1,user_coupon1,on=['user_id','coupon_id'],how='left')\n",
    "dataset1 = pd.merge(dataset1,other_feature1,on=['user_id','coupon_id','date_received','merchant_id'],how='left')\n",
    "dataset1.drop_duplicates(inplace=True)\n",
    "print (dataset1.shape)\n",
    "\n",
    "dataset1.user_merchant_buy_total = dataset1.user_merchant_buy_total.replace(np.nan,0)\n",
    "dataset1.user_merchant_any = dataset1.user_merchant_any.replace(np.nan,0)\n",
    "dataset1.user_merchant_received = dataset1.user_merchant_received.replace(np.nan,0)\n",
    "\n",
    "weekday_dummies = pd.get_dummies(dataset1.day_of_week)\n",
    "weekday_dummies.columns = ['weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset1 = pd.concat([dataset1,weekday_dummies],axis=1)\n",
    "dataset1['label'] = dataset1.date.astype('str') + ':' +  dataset1.date_received.astype('str')\n",
    "dataset1.label = dataset1.label.apply(get_label)\n",
    "dataset1.drop(['merchant_id','day_of_week','date','coupon_count'],axis=1,inplace=True)\n",
    "dataset1 = dataset1.replace('null',np.nan)\n",
    "dataset1.to_csv('data1/dataset1.csv',index=None)\n",
    "print(dataset1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
